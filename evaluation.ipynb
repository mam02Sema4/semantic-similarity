{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativa de Word2Vec, GloVe y FastText para obtener la similaridad semántica entre pares de textos\n",
    "## Pablo Valdunciel Sánchez \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el modelo *KeyedVectors* de la librería [*gesim*](https://radimrehurek.com/gensim/index.html) para cargar los vectores pre-entrenados de los diferentes modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los vectores pre-entrenados con modelos Word2Vec, GloVe y FastText. Los vectores pre-entrenados de cada modelo utilizados son:\n",
    "\n",
    "- **Word2Vec**:  [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)  \n",
    "- **GloVe**: [Common  Crawl  (840B  tokens,2.2M vocab, cased, 300d vectors)](http://nlp.stanford.edu/data/glove.840B.300d.zip)\n",
    "- **FastText**: [rawl-300d-2M.vec.zip:  2  million  word  vectors  trained  on  Common  Crawl  (600Btokens)](https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip)\n",
    "\n",
    "En el caso de los vectores de GloVe, se ha utilizado la función [*gensim.scripts.glove2word2vec.glove2word2vec*](https://radimrehurek.com/gensim/scripts/glove2word2vec.html) para convertir el archivo al formato Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WORD2VEC = './data/embedding/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "PATH_GLOVE = './data/embedding/glove/glove.840B.300d.w2v.txt'\n",
    "PATH_FASTTEXT = './data/embedding/fasttext/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los vectores puede llevar varios minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(PATH_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(PATH_GLOVE, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format(PATH_FASTTEXT, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de las funciones del módulo *load.py* cargamos los conjuntos de test de las tareas STS12, STS13, STS14, STS15 y STS16. Estas funciones llevan a cabo un preprocesamiento de las oraciones según los parámetros que se indiquen. Entre las posibilidades de preprocesamiento se encuentran:\n",
    "\n",
    "- **lowercase**: hacer que todas las palabras estén en minúscula.\n",
    "- **stop_words**: eliminar las palabras que no aportan casi significado semántico como determinantes, preprosiciones, etc.\n",
    "- **punctuation**: elminar los símbolos de puntuación.\n",
    "- **only_ascci**: eliminar las palabras que no estén formadas por caracteres ASCII.\n",
    "- **lemmatization**: sustituir las palabras por su lexema.\n",
    "\n",
    "El preprocesamiento del texto está implementado en la función *preprocess* en el módulo *utils.py*. La función *preprocess* hace uso de la librería [spaCy](https://spacy.io/) para llevar a cabo el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_sts_12, load_sts_13, load_sts_14, load_sts_15, load_sts_16\n",
    "from load import load_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no se aplica ningún tipo de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASETS = './data/datasets/STS'\n",
    "PATH_FREQUENCIES = './data/frequencies.tsv'\n",
    "PREPROCESSING =  {'lowercase':  False, \n",
    "                  'stop_words': False, \n",
    "                  'punctuation': False, \n",
    "                  'only_ascii': False, \n",
    "                  'lemmatization': False\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos también las frecuencias de las palabras en el corpus par poder aplicar el SIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = load_frequencies(PATH_FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** TASK: STS12 *****\n",
      "\n",
      "Preprocessing -MSRpar-\n",
      "-MSRpar- preprocessed correctly\n",
      "Preprocessing -MSRvid-\n",
      "-MSRvid- preprocessed correctly\n",
      "Preprocessing -SMTeuroparl-\n",
      "-SMTeuroparl- preprocessed correctly\n",
      "Preprocessing -surprise.OnWN-\n",
      "-surprise.OnWN- preprocessed correctly\n",
      "Preprocessing -surprise.SMTnews-\n",
      "-surprise.SMTnews- preprocessed correctly\n",
      "\n",
      "***** TASK: STS13 (-SMT) ***\n",
      "\n",
      "\n",
      "Preprocessing -FNWN-\n",
      "-FNWN- preprocessed correctly\n",
      "Preprocessing -headlines-\n",
      "-headlines- preprocessed correctly\n",
      "Preprocessing -OnWN-\n",
      "-OnWN- preprocessed correctly\n",
      "\n",
      "***** TASK: STS14 *****\n",
      "\n",
      "Preprocessing -deft-forum-\n",
      "-deft-forum- preprocessed correctly\n",
      "Preprocessing -deft-news-\n",
      "-deft-news- preprocessed correctly\n",
      "Preprocessing -headlines-\n",
      "-headlines- preprocessed correctly\n",
      "Preprocessing -images-\n",
      "-images- preprocessed correctly\n",
      "Preprocessing -OnWN-\n",
      "-OnWN- preprocessed correctly\n",
      "Preprocessing -tweet-news-\n",
      "-tweet-news- preprocessed correctly\n",
      "\n",
      "***** TASK: STS15 *****\n",
      "\n",
      "Preprocessing -answers-forums-\n",
      "-answers-forums- preprocessed correctly\n",
      "Preprocessing -answers-students-\n",
      "-answers-students- preprocessed correctly\n",
      "Preprocessing -belief-\n",
      "-belief- preprocessed correctly\n",
      "Preprocessing -headlines-\n",
      "-headlines- preprocessed correctly\n",
      "Preprocessing -images-\n",
      "-images- preprocessed correctly\n",
      "\n",
      "***** TASK: STS16 *****\n",
      "\n",
      "Preprocessing -answer-answer-\n",
      "-answer-answer- preprocessed correctly\n",
      "Preprocessing -headlines-\n",
      "-headlines- preprocessed correctly\n",
      "Preprocessing -plagiarism-\n",
      "-plagiarism- preprocessed correctly\n",
      "Preprocessing -postediting-\n",
      "-postediting- preprocessed correctly\n",
      "Preprocessing -question-question-\n",
      "-question-question- preprocessed correctly\n"
     ]
    }
   ],
   "source": [
    "sts12 = load_sts_12(PATH_DATASETS, PREPROCESSING)\n",
    "sts13 = load_sts_13(PATH_DATASETS, PREPROCESSING)\n",
    "sts14 = load_sts_14(PATH_DATASETS, PREPROCESSING)\n",
    "sts15 = load_sts_15(PATH_DATASETS, PREPROCESSING)\n",
    "sts16 = load_sts_16(PATH_DATASETS, PREPROCESSING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from methods import avg_cosine, wmd, sif_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = [\n",
    "    (\"Word2Vec + AVG\", partial(avg_cosine, model=word2vec)),\n",
    "    (\"Word2Vec + WMD\", partial(wmd, model=word2vec)),\n",
    "    (\"Word2Vec + SIF\", partial(sif_cosine, model=word2vec, frequencies=freqs, a=0.001))\n",
    "    #(\"GloVe + AVG\", partial(avg_cosine, model=glove)),\n",
    "    #(\"GloVe + WMD\", partial(wmd, model=glove)),\n",
    "    #(\"GloVe + SIF\", partial(sif_cosine, model=glove, frequencies=freqs, a=0.001)),\n",
    "    #(\"FastText + AVG\", partial(avg_cosine, model=fasttext)),\n",
    "    #(\"FastText + WMD\", partial(wmd, model=fasttext)),\n",
    "    #(\"FastText + SIF\", partial(sif_cosine, model=fasttext, frequencies=freqs, a=0.001))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sts12_pearson, sts12_spearman = evaluate(sts12, METHODS)\n",
    "word2vec_sts13_pearson, sts13_spearman = evaluate(sts13, METHODS)\n",
    "word2vec_sts14_pearson, sts14_spearman = evaluate(sts14, METHODS)\n",
    "word2vec_sts15_pearson, sts15_spearman = evaluate(sts15, METHODS)\n",
    "word2vec_sts16_pearson, sts16_spearman = evaluate(sts16, METHODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.6326237499753964,\n",
       " 'FastText + WMD': 0.6312212885648231,\n",
       " 'FastText + SIF': 0.7315395720676849}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_sts16_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Task STS12 ++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.6198524200427709,\n",
       " 'FastText + WMD': 0.5280347733615505,\n",
       " 'FastText + SIF': 0.6219464875863289}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++ Task STS12 ++++\")\n",
    "sts12_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Task STS13 ++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.690635771639227,\n",
       " 'FastText + WMD': 0.4012119583863098,\n",
       " 'FastText + SIF': 0.7557200532758551}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++ Task STS13 ++++\")\n",
    "sts13_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Task STS14 ++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.7264079006748914,\n",
       " 'FastText + WMD': 0.567738188273139,\n",
       " 'FastText + SIF': 0.7492322179215466}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++ Task STS14 ++++\")\n",
    "sts14_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Task STS15 ++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.7508655344369192,\n",
       " 'FastText + WMD': 0.6738544035294207,\n",
       " 'FastText + SIF': 0.7609634784830468}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++ Task STS15 ++++\")\n",
    "sts15_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Task STS16 ++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FastText + AVG': 0.7508655344369192,\n",
       " 'FastText + WMD': 0.6738544035294207,\n",
       " 'FastText + SIF': 0.7609634784830468}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++ Task STS16 ++++\")\n",
    "sts15_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
